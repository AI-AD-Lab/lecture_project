docker run --gpus all -it --name sam-dev -v $(pwd):/workspace  sam-dev  /bin/bash

docker exec -it sam-dev /bin/bash


adapive patch no training
Result:1.2761083562929467, Mean IOU:0.1798713339900892

vits
Result:0.2266345108010204, Mean IOU:0.8094880162951458


val data len: 1245
model_name=vit_h Phase=validation val_loss=0.3561  mIoU=0.7900  (402.4s)
model_name=vit_l Phase=validation val_loss=0.2153  mIoU=0.8362  (255.1s)
model_name=vit_b Phase=validation val_loss=0.4217  mIoU=0.7851  (125.7s)
model_name=vits Phase=validation val_loss=0.3662  mIoU=0.7858  (100.8s)
model_name=vitt Phase=validation val_loss=0.4579  mIoU=0.8031  (62.6s)






------------------------------------------------------------
vitt - last train epoch(30)
TRAIN || Loss - 0.007320320936273011, mIoU - 0.9919738318234881
TEST || Loss - 0.5167879357537347, mIoU - 0.9182885541417966
VAL || Loss - 1.3187689313448099, mIoU - 0.746185811240989

vitt - best result epoch(5)
TRAIN || Loss - 0.021857555717740562, mIoU - 0.9757094834536115
TEST || Loss - 0.172341336246038, mIoU - 0.9094273466744273
VAL || Loss - 0.33283126773724114, mIoU - 0.8120531985558659



인코딩 부분을 일정 크기로 통일된 상테
모든 vit 모델들의 인코딩 후 특징 크기: [B, 256, 64, 64]

segdecoder 처리 후에 나온 예측 mask 크기 
mask origin size: [1, 2, 256, 256]

posetPreceoss_mask 함수 이후 마스크 크기
mask resize size: [1, 2, 720, 1280] 

---------------------------
x size:torch.Size([1, 256, 64, 64])
local_res size: torch.Size([1, 256, 64, 64])

기존의 boundary mask 크기: [1, 1, 64, 64]

인코딩 데이터 크기로 broadcasting -> 1C를 64C로 확장
boundary_mask: torch.Size([1, 256, 64, 64])

score_8 size :torch.Size([1, 1, 128, 128])
score_16 szie: torch.Size([1, 1, 64, 64])
